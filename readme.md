## What is Inside:  
**AVDA_toy**:  
+ preliminary experiments testing the idea of masking features.
+ put /data with MNIST and SVHN data under it saves downloading time

**baselines**: 
+ baseline models;  
+ we also uses https://github.com/samotiian/CCSA as 
baseline&reference

**modified_FADA**: 
+ Applying an attention-like feature map
 on an adversarial-based VDA approach, FADA (in NIPS, 2017). 

**PVDA**: 
+ Experiments exploring the idea "Prototypical(Projection) Visual Domain Adaptation";  
+ Put /row_data from https://github.com/samotiian/CCSA under /PVDA to play;


## How to Use:
On its way...
