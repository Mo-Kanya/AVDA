{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p100 testing small batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "# from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FADAnet.FADAloader import *\n",
    "from FADAnet.FADAmodule import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss version: loss = loss1 + gamma*loss2/(10*n_support) + theta*loss3/(10*n_support)\n",
    "n_epoch = 201  # total number of epoch\n",
    "n_epoch_pt = 0\n",
    "batch_size = 128\n",
    "batch_size_test = 512\n",
    "lr = 0.0007\n",
    "gamma = 0.06\n",
    "theta = 0.05\n",
    "n_support = 7\n",
    "loss3_margin = 0.7\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "np.random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "torch.cuda.manual_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/SVHN/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "train_dataloader=mnist_dataloader(batch_size=batch_size,train=True)\n",
    "test_dataloader=mnist_dataloader(train=False)\n",
    "real_test_loader = svhn_dataloader(batch_size = batch_size_test, train=False)\n",
    "\n",
    "classifier=Classifier()\n",
    "encoder=Encoder()\n",
    "\n",
    "classifier.to(device)\n",
    "encoder.to(device)\n",
    "loss_fn1 = torch.nn.CrossEntropyLoss()\n",
    "loss_fn2 = torch.nn.CosineEmbeddingLoss()\n",
    "loss_fn3 = torch.nn.CosineEmbeddingLoss(margin=loss3_margin)\n",
    "optimizer=torch.optim.Adadelta(list(encoder.parameters())+list(classifier.parameters()))\n",
    "\n",
    "X_t, Y_t = create_target_samples(n=n_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1: 1.5125409364700317    loss2: 16.49027442932129    loss3: 0.0\n",
      "On source domain: Epoch 1/201  accuracy: 0.958 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 1/201  accuracy: 0.2154296875\n",
      "-------------------------------------------------\n",
      "loss1: 1.4869647026062012    loss2: 9.60530948638916    loss3: 0.007021208293735981\n",
      "loss1: 1.4713544845581055    loss2: 8.610066413879395    loss3: 0.01938006654381752\n",
      "On source domain: Epoch 11/201  accuracy: 0.987 \n",
      "loss1: 1.4611505270004272    loss2: 8.175106048583984    loss3: 0.0\n",
      "loss1: 1.4762845039367676    loss2: 7.679748058319092    loss3: 0.0917993113398552\n",
      "On source domain: Epoch 21/201  accuracy: 0.987 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 21/201  accuracy: 0.1431640625\n",
      "-------------------------------------------------\n",
      "loss1: 1.4611505270004272    loss2: 7.795801639556885    loss3: 0.19746613502502441\n",
      "loss1: 1.4611505270004272    loss2: 7.286019802093506    loss3: 0.004757311660796404\n",
      "On source domain: Epoch 31/201  accuracy: 0.990 \n",
      "loss1: 1.471567153930664    loss2: 7.627628803253174    loss3: 0.08497805893421173\n",
      "loss1: 1.4611505270004272    loss2: 6.833620071411133    loss3: 0.07906480133533478\n",
      "On source domain: Epoch 41/201  accuracy: 0.990 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 41/201  accuracy: 0.180859375\n",
      "-------------------------------------------------\n",
      "loss1: 1.4611505270004272    loss2: 7.4699883460998535    loss3: 0.018374888226389885\n",
      "loss1: 1.4611505270004272    loss2: 7.3455681800842285    loss3: 0.055032577365636826\n",
      "On source domain: Epoch 51/201  accuracy: 0.987 \n",
      "loss1: 1.481348991394043    loss2: 6.994419097900391    loss3: 0.10023495554924011\n",
      "loss1: 1.4653671979904175    loss2: 7.548407554626465    loss3: 0.0\n",
      "On source domain: Epoch 61/201  accuracy: 0.990 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 61/201  accuracy: 0.1248046875\n",
      "-------------------------------------------------\n",
      "loss1: 1.4611505270004272    loss2: 7.092996597290039    loss3: 0.09343680739402771\n",
      "loss1: 1.4616379737854004    loss2: 6.966521263122559    loss3: 0.1750836819410324\n",
      "On source domain: Epoch 71/201  accuracy: 0.990 \n",
      "loss1: 1.4611505270004272    loss2: 6.6771559715271    loss3: 0.09565378725528717\n",
      "loss1: 1.471567153930664    loss2: 6.558066368103027    loss3: 0.26307129859924316\n",
      "On source domain: Epoch 81/201  accuracy: 0.992 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 81/201  accuracy: 0.2734375\n",
      "-------------------------------------------------\n",
      "loss1: 1.4714502096176147    loss2: 7.666995048522949    loss3: 0.11854740977287292\n",
      "loss1: 1.4611505270004272    loss2: 7.27646541595459    loss3: 0.02007071115076542\n",
      "On source domain: Epoch 91/201  accuracy: 0.990 \n",
      "loss1: 1.4611505270004272    loss2: 7.059317588806152    loss3: 0.06426563858985901\n",
      "loss1: 1.4611505270004272    loss2: 6.357142925262451    loss3: 0.6367872357368469\n",
      "On source domain: Epoch 101/201  accuracy: 0.990 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 101/201  accuracy: 0.31640625\n",
      "-------------------------------------------------\n",
      "loss1: 1.4611505270004272    loss2: 6.733982086181641    loss3: 0.0015978151932358742\n",
      "loss1: 1.4611505270004272    loss2: 6.466034889221191    loss3: 0.1327827125787735\n",
      "On source domain: Epoch 111/201  accuracy: 0.989 \n",
      "loss1: 1.4611505270004272    loss2: 6.822188377380371    loss3: 0.03671383112668991\n",
      "loss1: 1.4611505270004272    loss2: 6.999720573425293    loss3: 0.18502743542194366\n",
      "On source domain: Epoch 121/201  accuracy: 0.992 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 121/201  accuracy: 0.3724609375\n",
      "-------------------------------------------------\n",
      "loss1: 1.4611505270004272    loss2: 6.163137912750244    loss3: 0.17708729207515717\n",
      "loss1: 1.4611505270004272    loss2: 6.054903507232666    loss3: 0.515771210193634\n",
      "On source domain: Epoch 131/201  accuracy: 0.991 \n",
      "loss1: 1.4611505270004272    loss2: 6.201848030090332    loss3: 0.41147711873054504\n",
      "loss1: 1.4611505270004272    loss2: 6.0908589363098145    loss3: 0.33207079768180847\n",
      "On source domain: Epoch 141/201  accuracy: 0.991 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 141/201  accuracy: 0.3390625\n",
      "-------------------------------------------------\n",
      "loss1: 1.471567153930664    loss2: 6.346414566040039    loss3: 0.06562697142362595\n",
      "loss1: 1.4611505270004272    loss2: 5.889694690704346    loss3: 0.11059047281742096\n",
      "On source domain: Epoch 151/201  accuracy: 0.991 \n",
      "loss1: 1.4611505270004272    loss2: 5.492586135864258    loss3: 0.024261826649308205\n",
      "loss1: 1.471567153930664    loss2: 5.1005539894104    loss3: 0.4172185957431793\n",
      "On source domain: Epoch 161/201  accuracy: 0.988 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 161/201  accuracy: 0.3837890625\n",
      "-------------------------------------------------\n",
      "loss1: 1.463205337524414    loss2: 5.760365009307861    loss3: 0.12730543315410614\n",
      "loss1: 1.4611505270004272    loss2: 5.3964996337890625    loss3: 0.35082581639289856\n",
      "On source domain: Epoch 171/201  accuracy: 0.988 \n",
      "loss1: 1.4715672731399536    loss2: 4.413041114807129    loss3: 0.21148517727851868\n",
      "loss1: 1.4611505270004272    loss2: 4.776501655578613    loss3: 0.11479635536670685\n",
      "On source domain: Epoch 181/201  accuracy: 0.989 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 181/201  accuracy: 0.416015625\n",
      "-------------------------------------------------\n",
      "loss1: 1.4715732336044312    loss2: 4.37705659866333    loss3: 0.1385854035615921\n",
      "loss1: 1.4713659286499023    loss2: 3.6770827770233154    loss3: 0.19973504543304443\n",
      "On source domain: Epoch 191/201  accuracy: 0.987 \n",
      "loss1: 1.4611783027648926    loss2: 3.7215118408203125    loss3: 0.14508862793445587\n",
      "loss1: 1.4611505270004272    loss2: 3.2684381008148193    loss3: 0.04945924133062363\n",
      "On source domain: Epoch 201/201  accuracy: 0.987 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 201/201  accuracy: 0.453515625\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# for epoch in tqdm(range(n_epoch)):\n",
    "for epoch in range(n_epoch):\n",
    "    \n",
    "    for data,labels in train_dataloader:\n",
    "        data=data.to(device)\n",
    "        labels=labels.to(device)\n",
    "        X_t = X_t.to(device)\n",
    "        Y_t = Y_t.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        map_s = encoder(data)\n",
    "        y_pred=classifier(map_s)\n",
    "        loss1=loss_fn1(y_pred,labels)\n",
    "        map_t = encoder(X_t)\n",
    "        \n",
    "        loss2 = 0\n",
    "        loss3 = 0\n",
    "        means_s = []\n",
    "        # means_t = []\n",
    "        \n",
    "        for num in range(10):\n",
    "#             subset = map_t[Y_t == num]\n",
    "#             means_t.append(torch.mean(subset, dim = 0))\n",
    "            subset = map_s[labels == num]\n",
    "            if len(subset) <= 1: \n",
    "                danger = 1\n",
    "            means_s.append(torch.mean(subset, dim = 0))\n",
    "        for ctr in range(10*n_support):\n",
    "            num = Y_t[ctr]\n",
    "            dd = torch.stack([ map_t[ctr] - means_s[num] ]*10)\n",
    "            Cplane = torch.stack( [(means_s[i-1] - means_s[i]) for i in range(10)] )\n",
    "            loss2 += loss_fn2(-dd, Cplane, torch.FloatTensor([-1]*10).to(device))\n",
    "            loss2 += loss_fn2(dd, Cplane, torch.FloatTensor([-1]*10).to(device))\n",
    "            Cplane = torch.stack( [( map_t[ctr] - means_s[i] ) for i in range(10) if i != num] )\n",
    "            loss3 += loss_fn3(dd[:-1], Cplane, torch.FloatTensor([-1]*9).to(device))\n",
    "        \n",
    "        if torch.isnan(loss1) or torch.isnan(loss2) or torch.isnan(loss3):\n",
    "            continue\n",
    "        loss = loss1 + gamma*loss2/(10*n_support) + theta*loss3/(10*n_support)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch%5 == 0: print(\"loss1:\", loss1.item(), \"   loss2:\", loss2.item(), \"   loss3:\", loss3.item())\n",
    "        \n",
    "    if epoch%10 == 0:\n",
    "        acc=0\n",
    "        for data,labels in test_dataloader:\n",
    "            data=data.to(device)\n",
    "            labels=labels.to(device)\n",
    "            y_test_pred=classifier(encoder(data))\n",
    "            acc+=(torch.max(y_test_pred,1)[1]==labels).float().mean().item()\n",
    "        accuracy=round(acc / float(len(test_dataloader)), 3)\n",
    "        print(\"On source domain: Epoch %d/%d  accuracy: %.3f \"%(epoch+1,n_epoch,accuracy))\n",
    "    \n",
    "    if epoch%20 == 0:\n",
    "        mapset_f = []\n",
    "        labelset_f = []\n",
    "        for data, labels in train_dataloader:\n",
    "            data = data.to(device)\n",
    "            labels=labels.to(device)\n",
    "            map_f = encoder(data)\n",
    "            mapset_f.append(map_f)\n",
    "            labelset_f.append(labels)\n",
    "        map_f = torch.cat(mapset_f[:-1])\n",
    "        label_f = torch.cat(labelset_f[:-1])\n",
    "\n",
    "        means_f = []\n",
    "        for num in range(10):\n",
    "            subset = map_f[label_f == num]\n",
    "            means_f.append(torch.mean(subset, dim = 0))\n",
    "#        nume = 0\n",
    "        deno = 0\n",
    "        acc = 0  \n",
    "        for data, labels in real_test_loader:\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)        \n",
    "            map_ff = encoder(data)\n",
    "            distTS = []\n",
    "            for ii in range(10):\n",
    "                distTS.append(torch.norm((map_ff - means_f[ii]), dim=1))\n",
    "            distTS = torch.stack(distTS)\n",
    "            acc+=torch.sum(torch.argmin(distTS, dim=0)==labels)\n",
    "            \n",
    "#             for ctr in range(batch_size_test):\n",
    "#                 others = []\n",
    "#                 for j in range(10):\n",
    "#                     num = j\n",
    "#                     tmp = map_ff[ctr] - means_f[num]\n",
    "#                     dd_f = torch.stack([tmp]*9)\n",
    "#                     tmp = [(means_f[num] - means_f[i]) for i in range(10) if i != num]\n",
    "#                     Cplane_f = torch.stack(tmp)\n",
    "#                     loss_f = loss_fn2(-dd_f, Cplane_f, torch.FloatTensor([-1]*9).to(device)) + loss_fn2(dd_f, Cplane_f, torch.FloatTensor([-1]*9).to(device))\n",
    "#                     others.append(loss_f.item())\n",
    "#                 # print(min(others))\n",
    "#                 if np.argmin(others) == labels[ctr]: \n",
    "#                     nume+=1\n",
    "            deno += batch_size_test\n",
    "            if deno > 5100: break\n",
    "        print(\"-------------------------------------------------\")\n",
    "#        print(\"On target domain: Epoch %d/%d  accuracy:\"%(epoch+1,n_epoch), nume / deno)\n",
    "        print(\"Another one on TD: Epoch %d/%d  accuracy:\"%(epoch+1,n_epoch), acc.item() / deno)\n",
    "        print(\"-------------------------------------------------\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1: 1.4681755304336548    loss2: 3.4909820556640625    loss3: 0.09560218453407288\n",
      "On source domain: Epoch 1/201  accuracy: 0.986 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 1/201  accuracy: 0.46796875\n",
      "-------------------------------------------------\n",
      "loss1: 1.4611505270004272    loss2: 3.119586944580078    loss3: 0.03658119589090347\n",
      "loss1: 1.4611505270004272    loss2: 2.426074981689453    loss3: 0.2116917371749878\n",
      "On source domain: Epoch 11/201  accuracy: 0.989 \n",
      "loss1: 1.4715681076049805    loss2: 2.5029056072235107    loss3: 0.31949055194854736\n",
      "loss1: 1.481950283050537    loss2: 2.9085214138031006    loss3: 0.01444072276353836\n",
      "On source domain: Epoch 21/201  accuracy: 0.987 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 21/201  accuracy: 0.405859375\n",
      "-------------------------------------------------\n",
      "loss1: 1.4611505270004272    loss2: 2.0377252101898193    loss3: 0.01241395901888609\n",
      "loss1: 1.4611505270004272    loss2: 2.019918203353882    loss3: 0.06268224865198135\n",
      "loss1: 1.4611505270004272    loss2: 1.8761961460113525    loss3: 0.01744774915277958\n",
      "On source domain: Epoch 41/201  accuracy: 0.989 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 41/201  accuracy: 0.4013671875\n",
      "-------------------------------------------------\n",
      "loss1: 1.4611505270004272    loss2: 1.8401457071304321    loss3: 0.20711195468902588\n",
      "loss1: 1.4611505270004272    loss2: 2.094653606414795    loss3: 0.0031274226494133472\n",
      "On source domain: Epoch 51/201  accuracy: 0.990 \n",
      "loss1: 1.4611505270004272    loss2: 1.5379855632781982    loss3: 0.05676325410604477\n",
      "loss1: 1.4611505270004272    loss2: 1.6070256233215332    loss3: 0.027254244312644005\n",
      "On source domain: Epoch 61/201  accuracy: 0.989 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 61/201  accuracy: 0.38828125\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4a12a616ce85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn_support\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss3\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn_support\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss1:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"   loss2:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"   loss3:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.6/site-packages/torch/optim/adadelta.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epoch):\n",
    "    \n",
    "    for data,labels in train_dataloader:\n",
    "        data=data.to(device)\n",
    "        labels=labels.to(device)\n",
    "        X_t = X_t.to(device)\n",
    "        Y_t = Y_t.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        map_s = encoder(data)\n",
    "        y_pred=classifier(map_s)\n",
    "        loss1=loss_fn1(y_pred,labels)\n",
    "        map_t = encoder(X_t)\n",
    "        \n",
    "        loss2 = 0\n",
    "        loss3 = 0\n",
    "        means_s = []\n",
    "        # means_t = []\n",
    "        \n",
    "        for num in range(10):\n",
    "#             subset = map_t[Y_t == num]\n",
    "#             means_t.append(torch.mean(subset, dim = 0))\n",
    "            subset = map_s[labels == num]\n",
    "            if len(subset) <= 1: \n",
    "                danger = 1\n",
    "            means_s.append(torch.mean(subset, dim = 0))\n",
    "        for ctr in range(10*n_support):\n",
    "            num = Y_t[ctr]\n",
    "            dd = torch.stack([ map_t[ctr] - means_s[num] ]*10)\n",
    "            Cplane = torch.stack( [(means_s[i-1] - means_s[i]) for i in range(10)] )\n",
    "            loss2 += loss_fn2(-dd, Cplane, torch.FloatTensor([-1]*10).to(device))\n",
    "            loss2 += loss_fn2(dd, Cplane, torch.FloatTensor([-1]*10).to(device))\n",
    "            Cplane = torch.stack( [( map_t[ctr] - means_s[i] ) for i in range(10) if i != num] )\n",
    "            loss3 += loss_fn3(dd[:-1], Cplane, torch.FloatTensor([-1]*9).to(device))\n",
    "        \n",
    "        if torch.isnan(loss1) or torch.isnan(loss2) or torch.isnan(loss3):\n",
    "            continue\n",
    "        loss = loss1 + gamma*loss2/(10*n_support) + theta*loss3/(10*n_support)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch%5 == 0: print(\"loss1:\", loss1.item(), \"   loss2:\", loss2.item(), \"   loss3:\", loss3.item())\n",
    "        \n",
    "    if epoch%10 == 0:\n",
    "        acc=0\n",
    "        for data,labels in test_dataloader:\n",
    "            data=data.to(device)\n",
    "            labels=labels.to(device)\n",
    "            y_test_pred=classifier(encoder(data))\n",
    "            acc+=(torch.max(y_test_pred,1)[1]==labels).float().mean().item()\n",
    "        accuracy=round(acc / float(len(test_dataloader)), 3)\n",
    "        print(\"On source domain: Epoch %d/%d  accuracy: %.3f \"%(epoch+1,n_epoch,accuracy))\n",
    "    \n",
    "    if epoch%20 == 0:\n",
    "        mapset_f = []\n",
    "        labelset_f = []\n",
    "        for data, labels in train_dataloader:\n",
    "            data = data.to(device)\n",
    "            labels=labels.to(device)\n",
    "            map_f = encoder(data)\n",
    "            mapset_f.append(map_f)\n",
    "            labelset_f.append(labels)\n",
    "        map_f = torch.cat(mapset_f[:-1])\n",
    "        label_f = torch.cat(labelset_f[:-1])\n",
    "\n",
    "        means_f = []\n",
    "        for num in range(10):\n",
    "            subset = map_f[label_f == num]\n",
    "            means_f.append(torch.mean(subset, dim = 0))\n",
    "#        nume = 0\n",
    "        deno = 0\n",
    "        acc = 0  \n",
    "        for data, labels in real_test_loader:\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)        \n",
    "            map_ff = encoder(data)\n",
    "            distTS = []\n",
    "            for ii in range(10):\n",
    "                distTS.append(torch.norm((map_ff - means_f[ii]), dim=1))\n",
    "            distTS = torch.stack(distTS)\n",
    "            acc+=torch.sum(torch.argmin(distTS, dim=0)==labels)\n",
    "            \n",
    "#             for ctr in range(batch_size_test):\n",
    "#                 others = []\n",
    "#                 for j in range(10):\n",
    "#                     num = j\n",
    "#                     tmp = map_ff[ctr] - means_f[num]\n",
    "#                     dd_f = torch.stack([tmp]*9)\n",
    "#                     tmp = [(means_f[num] - means_f[i]) for i in range(10) if i != num]\n",
    "#                     Cplane_f = torch.stack(tmp)\n",
    "#                     loss_f = loss_fn2(-dd_f, Cplane_f, torch.FloatTensor([-1]*9).to(device)) + loss_fn2(dd_f, Cplane_f, torch.FloatTensor([-1]*9).to(device))\n",
    "#                     others.append(loss_f.item())\n",
    "#                 # print(min(others))\n",
    "#                 if np.argmin(others) == labels[ctr]: \n",
    "#                     nume+=1\n",
    "            deno += batch_size_test\n",
    "            if deno > 5100: break\n",
    "        print(\"-------------------------------------------------\")\n",
    "#        print(\"On target domain: Epoch %d/%d  accuracy:\"%(epoch+1,n_epoch), nume / deno)\n",
    "        print(\"Another one on TD: Epoch %d/%d  accuracy:\"%(epoch+1,n_epoch), acc.item() / deno)\n",
    "        print(\"-------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.1745386  -10.011938    20.48194     -8.192516     8.475638\n",
      "  15.92413     -2.5822139   -0.3221105   23.022108    13.155584\n",
      "  -2.5788212    2.066813   -20.825943    15.767265    -3.7322679\n",
      "  11.519816     6.278767   -12.817459     4.203614    18.105707\n",
      "   6.2423835    8.375549   -17.613485   -20.151804    15.842639\n",
      "   7.831655    17.621256     2.5739157  -31.877438     4.3291283\n",
      "   0.7243002    9.685326    -3.6911      15.160153    -9.740432\n",
      "  21.500132    -6.8985434   -1.0789224  -10.07859    -11.179356\n",
      " -18.595795    11.691293    -4.747765    20.810911   -17.71167\n",
      " -22.918343    -7.025477     0.54644954  -5.8722315  -10.991843\n",
      "   9.204396    -1.2842579    4.2277765   -7.5103946    2.9380515\n",
      "  18.825861    56.05866      2.7505198   22.156355    -7.5402465\n",
      "   0.27074465  -8.500637    -6.7862105  -18.57417   ]\n",
      "0.0,\n",
      "153.1926,0.0,\n",
      "129.90042,135.44826,0.0,\n",
      "98.43348,91.81101,115.29391,0.0,\n",
      "123.75458,107.45866,100.80699,101.54213,0.0,\n",
      "155.96478,153.17664,145.15434,135.71869,144.40678,0.0,\n",
      "104.422195,110.87177,95.976875,86.154625,78.80514,131.45396,0.0,\n",
      "155.839,132.75533,120.32997,115.291,137.25671,168.69518,130.32825,0.0,\n",
      "128.75937,113.898384,107.28632,102.7049,102.637474,138.11545,92.244484,143.14877,0.0,\n",
      "134.73622,126.89183,117.53597,104.74394,108.08127,141.46565,94.693245,144.5607,121.6391,0.0,\n"
     ]
    }
   ],
   "source": [
    "# result check\n",
    "mapset = []\n",
    "labelset = []\n",
    "for data, labels in train_dataloader:\n",
    "    data=data.to(device)\n",
    "    fmap = encoder(data).cpu().detach().numpy()\n",
    "    labels=labels.to(device).cpu().detach().numpy()\n",
    "    mapset.append(fmap)\n",
    "    labelset.append(labels)\n",
    "\n",
    "smap = np.vstack(mapset[:-1])\n",
    "slabel = np.hstack(labelset[:-1])\n",
    "\n",
    "means = []\n",
    "dmeans = []\n",
    "\n",
    "for num in range(10):\n",
    "    subset1 = smap[slabel == num]\n",
    "    means1 = np.mean(subset1, axis=0)\n",
    "    tmp = subset1 - means1\n",
    "    dists1 = np.linalg.norm(tmp, axis=1)\n",
    "    means.append(means1)\n",
    "    dmeans.append(np.mean(dists1))\n",
    "print(means[0])\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(i+1):\n",
    "        print(np.linalg.norm(means[i] - means[j]), end=',')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15.62518, 12.894861, 17.233725, 14.968615, 14.968787, 22.451544, 13.444863, 20.46196, 17.3671, 17.675406]\n"
     ]
    }
   ],
   "source": [
    "tmapset = []\n",
    "tlabelset = []\n",
    "for data, labels in real_test_loader:\n",
    "    data=data.to(device)\n",
    "    fmap = encoder(data).cpu().detach().numpy()\n",
    "    labels=labels.to(device).cpu().detach().numpy()\n",
    "    tmapset.append(fmap)\n",
    "    tlabelset.append(labels)\n",
    "\n",
    "tmap = np.vstack(tmapset[:-1])\n",
    "tlabel = np.hstack(tlabelset[:-1])\n",
    "\n",
    "tmeans = []\n",
    "tdmeans = []\n",
    "for num in range(10):\n",
    "    subset1 = tmap[tlabel == num]\n",
    "    means1 = np.mean(subset1, axis=0)\n",
    "    tmp = subset1 - means1\n",
    "    dists1 = np.linalg.norm(tmp, axis=1)\n",
    "    tmeans.append(means1)\n",
    "    tdmeans.append(np.mean(dists1))\n",
    "print(dmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[109.98859, 145.80316, 133.12094, 112.94388, 127.58434, 158.94243, 113.93175, 154.0673, 129.75859, 132.459]\n",
      "[141.3419, 111.68559, 128.03937, 107.128395, 118.45304, 154.68042, 113.00985, 135.93904, 121.55752, 129.95499]\n",
      "[136.916, 135.47264, 104.95659, 116.27617, 115.841545, 153.81941, 109.78008, 136.67548, 120.065155, 125.03355]\n",
      "[130.44582, 124.80035, 123.91277, 101.12083, 115.628586, 141.42651, 104.802216, 142.43958, 116.96044, 115.77162]\n",
      "[124.11284, 123.036026, 117.15329, 102.746735, 99.60242, 148.26875, 96.80089, 143.62062, 115.26248, 113.04205]\n",
      "[143.78268, 138.88487, 133.3918, 119.83562, 126.583084, 123.85974, 114.255684, 156.35088, 126.716064, 123.576866]\n",
      "[124.64522, 132.73335, 121.918495, 108.7073, 114.166756, 142.40434, 98.1383, 148.37422, 114.488014, 116.86466]\n",
      "[162.33711, 146.72014, 136.16527, 132.16742, 146.58186, 174.18448, 138.7016, 123.67335, 148.63362, 151.2084]\n",
      "[129.11009, 130.04749, 122.73905, 108.84991, 114.781364, 146.04251, 103.91417, 148.26897, 108.80294, 119.76096]\n",
      "[131.437, 133.4105, 126.273575, 109.85321, 117.080505, 148.17773, 107.75093, 148.99403, 122.65551, 111.82491]\n"
     ]
    }
   ],
   "source": [
    "for num in range(10):\n",
    "    subset1 = tmap[tlabel == num]\n",
    "    tsd = []\n",
    "    for i in range(10):\n",
    "        tmp = np.linalg.norm((subset1 - means[i]), axis=1)\n",
    "        tsd.append(np.mean(tmp))\n",
    "    print(tsd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -5.034887    -9.137211     9.684193    -5.1165724    5.3417206\n",
      "   2.586344     1.9177574    2.8247616    4.3527427    4.5672827\n",
      "  -4.55892     -0.8631613   -3.876222     8.085945    -1.7921355\n",
      "  -1.4425997    0.84082437  -3.4799209   -1.1960568    7.2356153\n",
      "   1.5747296    5.0122123   -8.104763   -11.641367     6.3390903\n",
      "   5.321138    11.980459    -4.5846653   -5.2079797   -1.5730164\n",
      "  -3.1261814    3.9036183    3.83916      7.6159678  -12.081245\n",
      "   6.1791134    0.02005372  -1.3215011   -2.6292129   -5.601384\n",
      " -12.930199     1.1867195   -2.2178147    7.2312374   -7.9104977\n",
      "  -9.497583     5.0815043   -2.0728922   -9.720814    -3.7390308\n",
      "   4.165329    -0.86799586  -8.006124    -5.2386203    0.31658092\n",
      "   5.4438944  -14.283243     0.6545393    5.9680767   -4.4727\n",
      "   6.6153975   -8.814387    -6.071163    -4.3301086 ]\n"
     ]
    }
   ],
   "source": [
    "print(tmeans[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
