{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p100 testing new loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "# from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FADAnet.FADAloader import *\n",
    "from FADAnet.FADAmodule import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss version: loss = loss1 + gamma*loss2/(n_support*90) + theta*loss3/(10*n_support)\n",
    "n_epoch = 161  # total number of epoch\n",
    "n_epoch_pt = 0\n",
    "batch_size = 256\n",
    "batch_size_test = 512\n",
    "lr = 0.0007\n",
    "gamma = 0.06\n",
    "theta = 0.05\n",
    "n_support = 7\n",
    "loss3_margin = 0.7\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "np.random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "torch.cuda.manual_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/SVHN/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "train_dataloader=mnist_dataloader(batch_size=batch_size,train=True)\n",
    "test_dataloader=mnist_dataloader(train=False)\n",
    "real_test_loader = svhn_dataloader(batch_size = batch_size_test, train=False)\n",
    "\n",
    "classifier=Classifier()\n",
    "encoder=Encoder()\n",
    "\n",
    "classifier.to(device)\n",
    "encoder.to(device)\n",
    "loss_fn1 = torch.nn.CrossEntropyLoss()\n",
    "loss_fn2 = torch.nn.CosineEmbeddingLoss()\n",
    "loss_fn3 = torch.nn.CosineEmbeddingLoss(margin=loss3_margin)\n",
    "optimizer=torch.optim.Adadelta(list(encoder.parameters())+list(classifier.parameters()))\n",
    "\n",
    "X_t, Y_t = create_target_samples(n=n_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1: 1.5265402793884277    loss2: 354.2460021972656    loss3: 0.42317718267440796\n",
      "On source domain: Epoch 1/161  accuracy: 0.915 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 1/161  accuracy: 0.2140625\n",
      "-------------------------------------------------\n",
      "loss1: 1.493272304534912    loss2: 241.23873901367188    loss3: 2.2461657524108887\n",
      "loss1: 1.4715677499771118    loss2: 230.93429565429688    loss3: 1.6648293733596802\n",
      "loss1: 1.492607593536377    loss2: 223.9247283935547    loss3: 2.2680931091308594\n",
      "On source domain: Epoch 21/161  accuracy: 0.990 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 21/161  accuracy: 0.2388671875\n",
      "-------------------------------------------------\n",
      "loss1: 1.4611541032791138    loss2: 204.8499298095703    loss3: 1.7722448110580444\n",
      "loss1: 1.4611506462097168    loss2: 214.4391326904297    loss3: 1.4755756855010986\n",
      "loss1: 1.4612336158752441    loss2: 179.2998809814453    loss3: 0.977273166179657\n",
      "loss1: 1.4614208936691284    loss2: 153.70779418945312    loss3: 1.4024136066436768\n",
      "On source domain: Epoch 41/161  accuracy: 0.990 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 41/161  accuracy: 0.3265625\n",
      "-------------------------------------------------\n",
      "loss1: 1.4748554229736328    loss2: 101.7913818359375    loss3: 1.4598172903060913\n",
      "loss1: 1.4644631147384644    loss2: 73.86711883544922    loss3: 0.2805957496166229\n",
      "loss1: 1.4611693620681763    loss2: 50.813934326171875    loss3: 1.0779969692230225\n",
      "loss1: 1.4718599319458008    loss2: 45.20911407470703    loss3: 0.6277647018432617\n",
      "On source domain: Epoch 61/161  accuracy: 0.991 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 61/161  accuracy: 0.3220703125\n",
      "-------------------------------------------------\n",
      "loss1: 1.4611510038375854    loss2: 37.48667907714844    loss3: 0.3020937740802765\n",
      "loss1: 1.4611510038375854    loss2: 28.597013473510742    loss3: 0.2813717722892761\n",
      "loss1: 1.4699920415878296    loss2: 27.074247360229492    loss3: 0.2320089489221573\n",
      "loss1: 1.4611510038375854    loss2: 25.898412704467773    loss3: 0.1480419933795929\n",
      "On source domain: Epoch 81/161  accuracy: 0.989 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 81/161  accuracy: 0.336328125\n",
      "-------------------------------------------------\n",
      "loss1: 1.4611645936965942    loss2: 28.307344436645508    loss3: 0.8717848658561707\n",
      "loss1: 1.461157202720642    loss2: 21.19132423400879    loss3: 0.09096606820821762\n",
      "loss1: 1.4611668586730957    loss2: 18.75968360900879    loss3: 0.020956331863999367\n",
      "loss1: 1.4611506462097168    loss2: 22.225683212280273    loss3: 0.06086421012878418\n",
      "On source domain: Epoch 101/161  accuracy: 0.991 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 101/161  accuracy: 0.348828125\n",
      "-------------------------------------------------\n",
      "loss1: 1.461185097694397    loss2: 14.999883651733398    loss3: 0.060893524438142776\n",
      "loss1: 1.4611541032791138    loss2: 18.429502487182617    loss3: 0.24828575551509857\n",
      "loss1: 1.461151123046875    loss2: 14.7692232131958    loss3: 0.058958426117897034\n",
      "loss1: 1.4611505270004272    loss2: 17.993392944335938    loss3: 0.0015819735126569867\n",
      "On source domain: Epoch 121/161  accuracy: 0.991 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 121/161  accuracy: 0.3572265625\n",
      "-------------------------------------------------\n",
      "loss1: 1.4716171026229858    loss2: 20.062606811523438    loss3: 0.30548056960105896\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b8f511e9ec0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_support\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss3\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn_support\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for epoch in tqdm(range(n_epoch)):\n",
    "for epoch in range(n_epoch):\n",
    "    \n",
    "    for data,labels in train_dataloader:\n",
    "        data=data.to(device)\n",
    "        labels=labels.to(device)\n",
    "        X_t = X_t.to(device)\n",
    "        Y_t = Y_t.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        map_s = encoder(data)\n",
    "        y_pred=classifier(map_s)\n",
    "        loss1=loss_fn1(y_pred,labels)\n",
    "        map_t = encoder(X_t)\n",
    "        \n",
    "        loss2 = 0\n",
    "        loss3 = 0\n",
    "        means_s = []\n",
    "        # means_t = []\n",
    "        for num in range(10):\n",
    "#             subset = map_t[Y_t == num]\n",
    "#             means_t.append(torch.mean(subset, dim = 0))\n",
    "            subset = map_s[labels == num]\n",
    "            means_s.append(torch.mean(subset, dim = 0))\n",
    "        for ctr in range(10*n_support):\n",
    "            num = Y_t[ctr]\n",
    "            tmp = map_s[labels == num]\n",
    "            leng = len(tmp)\n",
    "            dd = torch.stack([ map_t[ctr] ]*leng) - tmp\n",
    "            for k in range(10):\n",
    "                if k == num: \n",
    "                    continue\n",
    "                Cplane = torch.stack( [means_s[k]]*leng ) - tmp\n",
    "                loss2 += loss_fn2(-dd, Cplane, torch.FloatTensor([-1]*leng).to(device))\n",
    "                loss2 += loss_fn2(dd, Cplane, torch.FloatTensor([-1]*leng).to(device))\n",
    "            \n",
    "            Cplane = torch.stack( [( map_t[ctr] - means_s[i] ) for i in range(10) if i != num] )\n",
    "            loss3 += loss_fn3(torch.stack([ map_t[ctr] - means_s[num] ]*9), Cplane, torch.FloatTensor([-1]*9).to(device))\n",
    "            \n",
    "        loss = loss1 + gamma*loss2/(n_support*90) + theta*loss3/(10*n_support)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch%5 == 0: print(\"loss1:\", loss1.item(), \"   loss2:\", loss2.item(), \"   loss3:\", loss3.item())\n",
    "        \n",
    "    if epoch%20 == 0:\n",
    "        acc=0\n",
    "        for data,labels in test_dataloader:\n",
    "            data=data.to(device)\n",
    "            labels=labels.to(device)\n",
    "            y_test_pred=classifier(encoder(data))\n",
    "            acc+=(torch.max(y_test_pred,1)[1]==labels).float().mean().item()\n",
    "        accuracy=round(acc / float(len(test_dataloader)), 3)\n",
    "        print(\"On source domain: Epoch %d/%d  accuracy: %.3f \"%(epoch+1,n_epoch,accuracy))\n",
    "    \n",
    "    if epoch%20 == 0:\n",
    "        mapset_f = []\n",
    "        labelset_f = []\n",
    "        for data, labels in train_dataloader:\n",
    "            data = data.to(device)\n",
    "            labels=labels.to(device)\n",
    "            map_f = encoder(data)\n",
    "            mapset_f.append(map_f)\n",
    "            labelset_f.append(labels)\n",
    "        map_f = torch.cat(mapset_f[:-1])\n",
    "        label_f = torch.cat(labelset_f[:-1])\n",
    "\n",
    "        means_f = []\n",
    "        for num in range(10):\n",
    "            subset = map_f[label_f == num]\n",
    "            means_f.append(torch.mean(subset, dim = 0))\n",
    "#        nume = 0\n",
    "        deno = 0\n",
    "        acc = 0\n",
    "        for data, labels in real_test_loader:\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            map_ff = encoder(data)\n",
    "            distTS = []\n",
    "            for ii in range(10):\n",
    "                distTS.append(torch.norm((map_ff - means_f[ii]), dim=1))\n",
    "            distTS = torch.stack(distTS)\n",
    "            acc+=torch.sum(torch.argmin(distTS, dim=0)==labels)\n",
    "\n",
    "#             for ctr in range(batch_size_test):\n",
    "#                 others = []\n",
    "#                 for j in range(10):\n",
    "#                     num = j\n",
    "#                     tmp = map_ff[ctr] - means_f[num]\n",
    "#                     dd_f = torch.stack([tmp]*9)\n",
    "#                     tmp = [(means_f[num] - means_f[i]) for i in range(10) if i != num]\n",
    "#                     Cplane_f = torch.stack(tmp)\n",
    "#                     loss_f = loss_fn2(-dd_f, Cplane_f, torch.FloatTensor([-1]*9).to(device)) + loss_fn2(dd_f, Cplane_f, torch.FloatTensor([-1]*9).to(device))\n",
    "#                     others.append(loss_f.item())\n",
    "#                 # print(min(others))\n",
    "#                 if np.argmin(others) == labels[ctr]: \n",
    "#                     nume+=1\n",
    "            deno += batch_size_test\n",
    "            if deno > 5100: break\n",
    "        print(\"-------------------------------------------------\")\n",
    "#        print(\"On target domain: Epoch %d/%d  accuracy:\"%(epoch+1,n_epoch), nume / deno)\n",
    "        print(\"Another one on TD: Epoch %d/%d  accuracy:\"%(epoch+1,n_epoch), acc.item() / deno)\n",
    "        print(\"-------------------------------------------------\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result check\n",
    "mapset = []\n",
    "labelset = []\n",
    "for data, labels in train_dataloader:\n",
    "    data=data.to(device)\n",
    "    fmap = encoder(data).cpu().detach().numpy()\n",
    "    labels=labels.to(device).cpu().detach().numpy()\n",
    "    mapset.append(fmap)\n",
    "    labelset.append(labels)\n",
    "\n",
    "smap = np.vstack(mapset[:-1])\n",
    "slabel = np.hstack(labelset[:-1])\n",
    "\n",
    "means = []\n",
    "dmeans = []\n",
    "\n",
    "for num in range(10):\n",
    "    subset1 = smap[slabel == num]\n",
    "    means1 = np.mean(subset1, axis=0)\n",
    "    tmp = subset1 - means1\n",
    "    dists1 = np.linalg.norm(tmp, axis=1)\n",
    "    means.append(means1)\n",
    "    dmeans.append(np.mean(dists1))\n",
    "print(means[0])\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(i+1):\n",
    "        print(np.linalg.norm(means[i] - means[j]), end=',')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52.963047, 51.995987, 68.22922, 49.21099, 66.8082, 51.826233, 60.485817, 47.8431, 64.16736, 48.692932]\n"
     ]
    }
   ],
   "source": [
    "tmapset = []\n",
    "tlabelset = []\n",
    "for data, labels in real_test_loader:\n",
    "    data=data.to(device)\n",
    "    fmap = encoder(data).cpu().detach().numpy()\n",
    "    labels=labels.to(device).cpu().detach().numpy()\n",
    "    tmapset.append(fmap)\n",
    "    tlabelset.append(labels)\n",
    "\n",
    "tmap = np.vstack(tmapset[:-1])\n",
    "tlabel = np.hstack(tlabelset[:-1])\n",
    "\n",
    "tmeans = []\n",
    "tdmeans = []\n",
    "for num in range(10):\n",
    "    subset1 = tmap[tlabel == num]\n",
    "    means1 = np.mean(subset1, axis=0)\n",
    "    tmp = subset1 - means1\n",
    "    dists1 = np.linalg.norm(tmp, axis=1)\n",
    "    tmeans.append(means1)\n",
    "    tdmeans.append(np.mean(dists1))\n",
    "print(dmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[445.79623, 505.58197, 498.05133, 483.19223, 498.62424, 489.73657, 513.93646, 466.88065, 504.06302, 465.8715]\n",
      "[489.79813, 477.8485, 517.3663, 541.3828, 500.82816, 517.25037, 545.397, 489.89743, 493.09137, 496.57635]\n",
      "[479.31104, 499.38995, 453.2038, 475.05606, 469.94528, 482.3067, 512.63184, 429.37158, 453.6323, 437.70984]\n",
      "[492.94138, 535.57776, 513.2114, 474.6914, 503.96408, 469.5211, 519.36365, 479.44348, 471.22406, 449.95145]\n",
      "[474.0837, 501.34534, 490.56793, 495.94513, 450.35468, 471.11, 486.37878, 477.05356, 462.7748, 443.85306]\n",
      "[466.91852, 511.1242, 510.3899, 456.1864, 459.6347, 386.33572, 480.76993, 458.69843, 425.5686, 413.47714]\n",
      "[450.6405, 501.76154, 472.65945, 450.88358, 459.22675, 431.38123, 459.27383, 457.92798, 442.16064, 436.3949]\n",
      "[463.87222, 476.37033, 472.42426, 481.8027, 481.32196, 483.51425, 543.73456, 397.78662, 464.65823, 432.1347]\n",
      "[493.98096, 516.72516, 500.42645, 486.48898, 480.67206, 458.1011, 498.9799, 485.12332, 443.25677, 451.43924]\n",
      "[482.16678, 524.95776, 522.28467, 491.80377, 477.67206, 454.98584, 514.72705, 479.3818, 471.5705, 431.8555]\n"
     ]
    }
   ],
   "source": [
    "for num in range(10):\n",
    "    subset1 = tmap[tlabel == num]\n",
    "    tsd = []\n",
    "    for i in range(10):\n",
    "        tmp = np.linalg.norm((subset1 - means[i]), axis=1)\n",
    "        tsd.append(np.mean(tmp))\n",
    "    print(tsd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -31.16322     86.0356      13.188254   -29.734535   -60.355446\n",
      "   17.576384   134.30591     79.11561     37.532696   -11.958997\n",
      " -123.304985     3.1236432   25.998098   -17.795744     6.428827\n",
      "   48.07155     16.341524   -58.37026     27.04379     46.83075\n",
      "    7.5277233  -83.45277    -41.75638     66.62036     10.13417\n",
      "   20.246445    -0.355838  -125.27577     66.08496      4.564584\n",
      "   13.589962    46.206104    -3.8616078   33.55046    -59.034954\n",
      "   39.694664   116.03252    -82.125694   -13.166707   -11.973564\n",
      "  -43.58825    -41.531235    23.7037      51.590103   -29.155138\n",
      "   36.005238   -23.759588   -20.057531   -15.161339    10.573125\n",
      "  -74.27094    -15.096857   -83.16608    -35.028885   -89.51028\n",
      "  -24.386696   -25.232468   129.88492     38.813408    37.56294\n",
      "   15.895474   -26.870974  -179.83022     71.17689  ]\n"
     ]
    }
   ],
   "source": [
    "print(tmeans[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
