{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "# from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FADAnet.S2Mloader import *\n",
    "from FADAnet.FADAmodule import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss version: loss = loss1 + gamma*loss2/(10*n_support) + theta*loss3/(10*n_support)\n",
    "n_epoch = 161  # total number of epoch\n",
    "n_epoch_pt = 0\n",
    "batch_size = 256\n",
    "batch_size_test = 512\n",
    "lr = 0.0007\n",
    "gamma = 0.05\n",
    "theta = 0.04\n",
    "n_support = 7\n",
    "loss3_margin = 0.7\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "np.random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "torch.cuda.manual_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: ./data/SVHN/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "train_dataloader=svhn_dataloader(batch_size=batch_size,train=True)\n",
    "test_dataloader=svhn_dataloader(train=False)\n",
    "real_test_loader = mnist_dataloader(batch_size = batch_size_test, train=False)\n",
    "\n",
    "classifier=Classifier()\n",
    "encoder=Encoder()\n",
    "\n",
    "classifier.to(device)\n",
    "encoder.to(device)\n",
    "loss_fn1 = torch.nn.CrossEntropyLoss()\n",
    "loss_fn2 = torch.nn.CosineEmbeddingLoss()\n",
    "loss_fn3 = torch.nn.CosineEmbeddingLoss(margin=loss3_margin)\n",
    "optimizer=torch.optim.Adadelta(list(encoder.parameters())+list(classifier.parameters()))\n",
    "\n",
    "X_t, Y_t = create_target_samples(n=n_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1: 2.2746148109436035    loss2: 1.6422038078308105    loss3: 17.466873168945312\n",
      "On source domain: Epoch 1/161  accuracy: 0.196 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 1/161  accuracy: 0.1294921875\n",
      "-------------------------------------------------\n",
      "loss1: 1.6324539184570312    loss2: 14.774789810180664    loss3: 1.2258985042572021\n",
      "loss1: 1.578744888305664    loss2: 11.0239839553833    loss3: 0.4115108549594879\n",
      "On source domain: Epoch 11/161  accuracy: 0.774 \n",
      "loss1: 1.5579211711883545    loss2: 8.534467697143555    loss3: 0.21657395362854004\n",
      "loss1: 1.5798814296722412    loss2: 7.406755447387695    loss3: 0.619644045829773\n",
      "On source domain: Epoch 21/161  accuracy: 0.803 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 21/161  accuracy: 0.7271484375\n",
      "-------------------------------------------------\n",
      "loss1: 1.5007237195968628    loss2: 9.710536003112793    loss3: 0.1150103509426117\n",
      "loss1: 1.4876405000686646    loss2: 7.993405818939209    loss3: 0.22610610723495483\n",
      "On source domain: Epoch 31/161  accuracy: 0.867 \n",
      "loss1: 1.5098206996917725    loss2: 6.709917068481445    loss3: 0.1653439700603485\n",
      "loss1: 1.5099889039993286    loss2: 6.689051151275635    loss3: 0.2736150324344635\n",
      "On source domain: Epoch 41/161  accuracy: 0.869 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 41/161  accuracy: 0.73984375\n",
      "-------------------------------------------------\n",
      "loss1: 1.4885896444320679    loss2: 6.001312255859375    loss3: 0.18246451020240784\n",
      "loss1: 1.5124497413635254    loss2: 6.7237372398376465    loss3: 0.06042652577161789\n",
      "On source domain: Epoch 51/161  accuracy: 0.880 \n",
      "loss1: 1.4855477809906006    loss2: 6.220250606536865    loss3: 0.31255650520324707\n",
      "loss1: 1.5099310874938965    loss2: 5.529173374176025    loss3: 0.16052180528640747\n",
      "On source domain: Epoch 61/161  accuracy: 0.877 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 61/161  accuracy: 0.7892578125\n",
      "-------------------------------------------------\n",
      "loss1: 1.4855408668518066    loss2: 5.87382173538208    loss3: 0.018104607239365578\n",
      "loss1: 1.5587074756622314    loss2: 5.828916072845459    loss3: 0.04827047511935234\n",
      "On source domain: Epoch 71/161  accuracy: 0.877 \n",
      "loss1: 1.4855350255966187    loss2: 6.901581764221191    loss3: 0.011712703853845596\n",
      "On source domain: Epoch 81/161  accuracy: 0.883 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 81/161  accuracy: 0.7744140625\n",
      "-------------------------------------------------\n",
      "loss1: 1.5193910598754883    loss2: 5.861653804779053    loss3: 0.12354296445846558\n",
      "loss1: 1.4850627183914185    loss2: 6.002906322479248    loss3: 0.09815483540296555\n",
      "On source domain: Epoch 91/161  accuracy: 0.881 \n",
      "loss1: 1.485540747642517    loss2: 5.716881275177002    loss3: 0.08864867687225342\n",
      "loss1: 1.4855408668518066    loss2: 6.177761554718018    loss3: 0.06748809665441513\n",
      "On source domain: Epoch 101/161  accuracy: 0.881 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 101/161  accuracy: 0.7955078125\n",
      "-------------------------------------------------\n",
      "loss1: 1.5099306106567383    loss2: 5.585806369781494    loss3: 0.02797926962375641\n",
      "loss1: 1.4855408668518066    loss2: 6.230380535125732    loss3: 0.0\n",
      "On source domain: Epoch 111/161  accuracy: 0.885 \n",
      "loss1: 1.509924054145813    loss2: 6.160336971282959    loss3: 0.08390304446220398\n",
      "loss1: 1.4855408668518066    loss2: 5.835670471191406    loss3: 0.024672985076904297\n",
      "On source domain: Epoch 121/161  accuracy: 0.884 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 121/161  accuracy: 0.8048828125\n",
      "-------------------------------------------------\n",
      "loss1: 1.493546724319458    loss2: 5.753085136413574    loss3: 0.015264345332980156\n",
      "loss1: 1.4855408668518066    loss2: 6.00968074798584    loss3: 0.09559372067451477\n",
      "On source domain: Epoch 131/161  accuracy: 0.887 \n",
      "loss1: 1.4855408668518066    loss2: 6.014954566955566    loss3: 0.03620132431387901\n",
      "loss1: 1.5343214273452759    loss2: 6.069806098937988    loss3: 0.030413486063480377\n",
      "On source domain: Epoch 141/161  accuracy: 0.884 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 141/161  accuracy: 0.7892578125\n",
      "-------------------------------------------------\n",
      "loss1: 1.4855408668518066    loss2: 5.336387634277344    loss3: 0.08111731708049774\n",
      "loss1: 1.4855417013168335    loss2: 4.998266220092773    loss3: 0.005758325569331646\n",
      "On source domain: Epoch 151/161  accuracy: 0.887 \n",
      "loss1: 1.5331519842147827    loss2: 5.9636430740356445    loss3: 0.04666772857308388\n",
      "loss1: 1.4855403900146484    loss2: 5.860832691192627    loss3: 0.054739437997341156\n",
      "On source domain: Epoch 161/161  accuracy: 0.888 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 161/161  accuracy: 0.79296875\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# for epoch in tqdm(range(n_epoch)):\n",
    "for epoch in range(n_epoch):\n",
    "    \n",
    "    for data,labels in train_dataloader:\n",
    "        data=data.to(device)\n",
    "        labels=labels.to(device)\n",
    "        X_t = X_t.to(device)\n",
    "        Y_t = Y_t.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        map_s = encoder(data)\n",
    "        y_pred=classifier(map_s)\n",
    "        loss1=loss_fn1(y_pred,labels)\n",
    "        map_t = encoder(X_t)\n",
    "        \n",
    "        loss2 = 0\n",
    "        loss3 = 0\n",
    "        means_s = []\n",
    "        # means_t = []\n",
    "        for num in range(10):\n",
    "#             subset = map_t[Y_t == num]\n",
    "#             means_t.append(torch.mean(subset, dim = 0))\n",
    "            subset = map_s[labels == num]\n",
    "            means_s.append(torch.mean(subset, dim = 0))\n",
    "        for ctr in range(10*n_support):\n",
    "            num = Y_t[ctr]\n",
    "            dd = torch.stack([ map_t[ctr] - means_s[num] ]*10)\n",
    "            Cplane = torch.stack( [(means_s[i-1] - means_s[i]) for i in range(10)] )\n",
    "            loss2 += loss_fn2(-dd, Cplane, torch.FloatTensor([-1]*10).to(device))\n",
    "            loss2 += loss_fn2(dd, Cplane, torch.FloatTensor([-1]*10).to(device))\n",
    "            Cplane = torch.stack( [( map_t[ctr] - means_s[i] ) for i in range(10) if i != num] )\n",
    "            loss3 += loss_fn3(dd[:-1], Cplane, torch.FloatTensor([-1]*9).to(device))\n",
    "            \n",
    "        loss = loss1 + gamma*loss2/(10*n_support) + theta*loss3/(10*n_support)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch%5 == 0: print(\"loss1:\", loss1.item(), \"   loss2:\", loss2.item(), \"   loss3:\", loss3.item())\n",
    "        \n",
    "    if epoch%10 == 0:\n",
    "        acc=0\n",
    "        for data,labels in test_dataloader:\n",
    "            data=data.to(device)\n",
    "            labels=labels.to(device)\n",
    "            y_test_pred=classifier(encoder(data))\n",
    "            acc+=(torch.max(y_test_pred,1)[1]==labels).float().mean().item()\n",
    "        accuracy=round(acc / float(len(test_dataloader)), 3)\n",
    "        print(\"On source domain: Epoch %d/%d  accuracy: %.3f \"%(epoch+1,n_epoch,accuracy))\n",
    "    \n",
    "    if epoch%20 == 0:\n",
    "        mapset_f = []\n",
    "        labelset_f = []\n",
    "        for data, labels in train_dataloader:\n",
    "            data = data.to(device)\n",
    "            labels=labels.to(device)\n",
    "            map_f = encoder(data)\n",
    "            mapset_f.append(map_f)\n",
    "            labelset_f.append(labels)\n",
    "        map_f = torch.cat(mapset_f[:-1])\n",
    "        label_f = torch.cat(labelset_f[:-1])\n",
    "\n",
    "        means_f = []\n",
    "        for num in range(10):\n",
    "            subset = map_f[label_f == num]\n",
    "            means_f.append(torch.mean(subset, dim = 0))\n",
    "#         nume = 0\n",
    "        deno = 0\n",
    "        acc = 0  \n",
    "        for data, labels in real_test_loader:\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)        \n",
    "            map_ff = encoder(data)\n",
    "            distTS = []\n",
    "            for ii in range(10):\n",
    "                distTS.append(torch.norm((map_ff - means_f[ii]), dim=1))\n",
    "            distTS = torch.stack(distTS)\n",
    "            acc+=torch.sum(torch.argmin(distTS, dim=0)==labels)\n",
    "            \n",
    "#             for ctr in range(batch_size_test):\n",
    "#                 others = []\n",
    "#                 for j in range(10):\n",
    "#                     num = j\n",
    "#                     tmp = map_ff[ctr] - means_f[num]\n",
    "#                     dd_f = torch.stack([tmp]*9)\n",
    "#                     tmp = [(means_f[num] - means_f[i]) for i in range(10) if i != num]\n",
    "#                     Cplane_f = torch.stack(tmp)\n",
    "#                     loss_f = loss_fn2(-dd_f, Cplane_f, torch.FloatTensor([-1]*9).to(device)) + loss_fn2(dd_f, Cplane_f, torch.FloatTensor([-1]*9).to(device))\n",
    "#                     others.append(loss_f.item())\n",
    "#                 # print(min(others))\n",
    "#                 if np.argmin(others) == labels[ctr]: \n",
    "#                     nume+=1\n",
    "            deno += batch_size_test\n",
    "            if deno > 5100: break\n",
    "        print(\"-------------------------------------------------\")\n",
    "        # print(\"On target domain: Epoch %d/%d  accuracy:\"%(epoch+1,n_epoch), nume / deno)\n",
    "        print(\"Another one on TD: Epoch %d/%d  accuracy:\"%(epoch+1,n_epoch), acc.item() / deno)\n",
    "        print(\"-------------------------------------------------\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-23.688744    21.273893     3.1445177    1.1988074    6.0620027\n",
      " -11.410247    27.245071    -7.2591133  -10.65264    -28.3089\n",
      " -22.033226     2.842914   -14.670648     6.21544     42.38876\n",
      "  39.800205    24.564138     7.4917865   34.946407     8.950566\n",
      "  28.74279     -0.63787794  18.698229   -35.204746   -16.776817\n",
      "   9.953578   -12.181389    18.404148    14.666326   -19.785471\n",
      " -15.118991     6.8794265  -13.031214     0.9114728  -24.350803\n",
      "  27.645535     2.0251348    3.2660334  -23.596907    14.472404\n",
      " -12.164628    60.08781     42.950176    24.292904    -7.956047\n",
      " -38.490963   -14.454111    33.338936    24.302477   -25.859348\n",
      "  16.611355   -28.318966   -23.777386    27.504766    36.42862\n",
      "  10.824631   -19.561558   -40.200413    49.791332     5.3936415\n",
      " -19.12181    -52.97179      0.12562715 -10.164382  ]\n",
      "0.0,\n",
      "236.48732,0.0,\n",
      "287.08545,259.55295,0.0,\n",
      "275.6476,243.73723,251.43686,0.0,\n",
      "296.27084,231.9413,259.86368,260.98044,0.0,\n",
      "323.48795,292.15512,302.3135,250.78674,289.3931,0.0,\n",
      "264.2199,297.60117,320.15494,296.37213,275.03473,243.47319,0.0,\n",
      "242.98906,211.46674,238.32878,225.67883,233.15703,261.17505,260.48013,0.0,\n",
      "263.5133,254.98463,271.27625,232.09256,259.5735,286.4051,243.91719,222.82373,0.0,\n",
      "221.27893,246.11722,264.10007,245.84193,234.82231,246.29308,312.1753,210.11555,218.49342,0.0,\n"
     ]
    }
   ],
   "source": [
    "# result check\n",
    "mapset = []\n",
    "labelset = []\n",
    "for data, labels in train_dataloader:\n",
    "    data=data.to(device)\n",
    "    fmap = encoder(data).cpu().detach().numpy()\n",
    "    labels=labels.to(device).cpu().detach().numpy()\n",
    "    mapset.append(fmap)\n",
    "    labelset.append(labels)\n",
    "\n",
    "smap = np.vstack(mapset[:-1])\n",
    "slabel = np.hstack(labelset[:-1])\n",
    "\n",
    "means = []\n",
    "dmeans = []\n",
    "\n",
    "for num in range(10):\n",
    "    subset1 = smap[slabel == num]\n",
    "    means1 = np.mean(subset1, axis=0)\n",
    "    tmp = subset1 - means1\n",
    "    dists1 = np.linalg.norm(tmp, axis=1)\n",
    "    means.append(means1)\n",
    "    dmeans.append(np.mean(dists1))\n",
    "print(means[0])\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(i+1):\n",
    "        print(np.linalg.norm(means[i] - means[j]), end=',')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[131.37401, 106.545456, 127.75493, 113.06761, 117.81129, 142.30075, 150.75305, 78.9538, 125.301, 115.88948]\n"
     ]
    }
   ],
   "source": [
    "tmapset = []\n",
    "tlabelset = []\n",
    "for data, labels in real_test_loader:\n",
    "    data=data.to(device)\n",
    "    fmap = encoder(data).cpu().detach().numpy()\n",
    "    labels=labels.to(device).cpu().detach().numpy()\n",
    "    tmapset.append(fmap)\n",
    "    tlabelset.append(labels)\n",
    "\n",
    "tmap = np.vstack(tmapset[:-1])\n",
    "tlabel = np.hstack(tlabelset[:-1])\n",
    "\n",
    "tmeans = []\n",
    "tdmeans = []\n",
    "for num in range(10):\n",
    "    subset1 = tmap[tlabel == num]\n",
    "    means1 = np.mean(subset1, axis=0)\n",
    "    tmp = subset1 - means1\n",
    "    dists1 = np.linalg.norm(tmp, axis=1)\n",
    "    tmeans.append(means1)\n",
    "    tdmeans.append(np.mean(dists1))\n",
    "print(dmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290.9956, 373.67953, 389.38113, 393.7371, 405.95767, 426.6419, 387.4414, 376.52304, 387.43335, 363.84274]\n",
      "[323.97153, 230.49045, 322.41327, 322.7934, 305.6804, 358.49924, 363.0555, 292.81622, 327.4344, 322.03897]\n",
      "[436.72092, 413.77103, 300.74545, 398.9159, 403.47165, 435.0979, 451.06293, 394.66595, 422.61343, 418.6802]\n",
      "[379.4224, 360.74832, 362.70453, 263.1432, 368.55246, 357.39172, 387.0257, 346.47733, 348.38406, 361.4477]\n",
      "[409.5532, 364.7748, 380.37994, 382.64645, 272.24857, 399.53793, 396.94843, 364.98962, 386.66483, 357.1262]\n",
      "[469.2989, 448.62766, 454.87463, 405.29953, 436.79926, 294.37714, 372.53497, 428.3165, 433.06668, 425.2241]\n",
      "[376.67355, 375.45593, 380.82263, 370.85666, 325.3451, 353.11145, 302.06384, 359.21637, 359.46265, 380.14117]\n",
      "[355.14615, 331.12982, 346.1563, 338.7127, 341.7284, 355.0337, 364.51785, 239.53, 344.7452, 327.47513]\n",
      "[365.9263, 354.0274, 358.7895, 324.70456, 352.53604, 361.3994, 341.49405, 329.69452, 258.12878, 329.79816]\n",
      "[326.7711, 326.8759, 333.70468, 330.44034, 304.62186, 325.1944, 367.697, 301.6928, 316.95828, 249.11005]\n"
     ]
    }
   ],
   "source": [
    "for num in range(10):\n",
    "    subset1 = tmap[tlabel == num]\n",
    "    tsd = []\n",
    "    for i in range(10):\n",
    "        tmp = np.linalg.norm((subset1 - means[i]), axis=1)\n",
    "        tsd.append(np.mean(tmp))\n",
    "    print(tsd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-25.745762    15.9033985    4.660685    -3.5865479  -51.985085\n",
      " -75.84789     69.041664   -24.451033    36.76213     16.207006\n",
      " -49.550293    18.481579   -33.656685   -22.327173   -26.14766\n",
      "   4.639961    -0.1664241  -16.648289    85.47253    -12.875888\n",
      "  -5.276546   -12.34953     -8.334483   -45.763218   -20.043188\n",
      "  11.530189    -3.2756162   13.591866    39.04248     -5.4091463\n",
      " -12.622531    -4.3483872  -13.063196    35.20257    -21.803411\n",
      "  36.26048    -29.631714     2.0178175   -9.971764   -12.942284\n",
      "  14.583926    96.17354     62.914017    81.518      -34.537804\n",
      " -33.287106   -63.502804    61.22401    -36.03754    -58.169857\n",
      "   9.957944   -36.862183   -26.718363    28.1729      31.984795\n",
      "   6.60877     31.57246    -97.49038     72.86181     40.584488\n",
      "  -0.12022278 -33.237915    40.598946     5.8581066 ]\n"
     ]
    }
   ],
   "source": [
    "print(tmeans[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
