{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "# from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FADAnet.UMloader import *\n",
    "from FADAnet.UMmodule import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss version: loss = loss1 + gamma*loss2/(10*n_support) + theta*loss3/(10*n_support)\n",
    "n_epoch = 161  # total number of epoch\n",
    "n_epoch_pt = 0\n",
    "batch_size = 128\n",
    "batch_size_test = 256\n",
    "lr = 0.0007\n",
    "gamma = 0.065\n",
    "theta = 0.055\n",
    "n_support = 7\n",
    "loss3_margin = 0.7\n",
    "\n",
    "domain_adaptation_task = 'MNIST_to_USPS'\n",
    "repetition = 0\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "np.random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "torch.cuda.manual_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source X :  2000  Y :  2000\n",
      "Target X :  70  Y :  70\n",
      "Class P :  14000  N :  126000\n"
     ]
    }
   ],
   "source": [
    "train_set = TrainSet(domain_adaptation_task, repetition, n_support)\n",
    "test_set = TestSet(domain_adaptation_task, repetition, n_support)\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "real_test_loader = DataLoader(test_set, batch_size=batch_size_test, shuffle=True, drop_last=True)\n",
    "\n",
    "classifier=Classifier()\n",
    "encoder=Encoder()\n",
    "classifier.to(device)\n",
    "encoder.to(device)\n",
    "\n",
    "loss_fn1 = torch.nn.CrossEntropyLoss()\n",
    "loss_fn2 = torch.nn.CosineEmbeddingLoss()\n",
    "loss_fn3 = torch.nn.CosineEmbeddingLoss(margin=loss3_margin)\n",
    "optimizer=torch.optim.Adadelta(list(encoder.parameters())+list(classifier.parameters()))\n",
    "\n",
    "X_t = torch.from_numpy(train_set.x_target).unsqueeze(1)\n",
    "Y_t = torch.from_numpy(train_set.y_target).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1: 1.4972822666168213    loss2: 14.995403289794922    loss3: 0.0\n",
      "On source domain: Epoch 1/161  accuracy: 0.952 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 1/161  accuracy: 0.6194196428571429\n",
      "-------------------------------------------------\n",
      "loss1: 1.4717826843261719    loss2: 8.824933052062988    loss3: 0.07994669675827026\n",
      "loss1: 1.4704762697219849    loss2: 5.719654083251953    loss3: 0.6001102328300476\n",
      "On source domain: Epoch 11/161  accuracy: 0.995 \n",
      "loss1: 1.4651024341583252    loss2: 3.956559419631958    loss3: 0.3133990168571472\n",
      "loss1: 1.4766459465026855    loss2: 3.198007345199585    loss3: 0.47490960359573364\n",
      "On source domain: Epoch 21/161  accuracy: 0.997 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 21/161  accuracy: 0.8699776785714286\n",
      "-------------------------------------------------\n",
      "loss1: 1.461299180984497    loss2: 3.0450539588928223    loss3: 0.35706719756126404\n",
      "loss1: 1.4689639806747437    loss2: 2.7535400390625    loss3: 0.14976628124713898\n",
      "On source domain: Epoch 31/161  accuracy: 0.997 \n",
      "loss1: 1.4612187147140503    loss2: 2.5530405044555664    loss3: 0.3301144540309906\n",
      "loss1: 1.4611529111862183    loss2: 2.3628578186035156    loss3: 0.24743780493736267\n",
      "On source domain: Epoch 41/161  accuracy: 0.998 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 41/161  accuracy: 0.8883928571428571\n",
      "-------------------------------------------------\n",
      "loss1: 1.4611586332321167    loss2: 2.3260347843170166    loss3: 0.14886623620986938\n",
      "loss1: 1.4611519575119019    loss2: 1.9090527296066284    loss3: 0.48850247263908386\n",
      "On source domain: Epoch 51/161  accuracy: 0.999 \n",
      "loss1: 1.4692796468734741    loss2: 2.054593563079834    loss3: 0.39230358600616455\n",
      "loss1: 1.4620285034179688    loss2: 1.566857099533081    loss3: 0.3597125709056854\n",
      "On source domain: Epoch 61/161  accuracy: 0.999 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 61/161  accuracy: 0.8967633928571429\n",
      "-------------------------------------------------\n",
      "loss1: 1.4611507654190063    loss2: 1.5059162378311157    loss3: 0.42779242992401123\n",
      "loss1: 1.4611527919769287    loss2: 1.8940134048461914    loss3: 0.2771318554878235\n",
      "On source domain: Epoch 71/161  accuracy: 0.999 \n",
      "loss1: 1.4611525535583496    loss2: 1.8076744079589844    loss3: 0.296724796295166\n",
      "loss1: 1.4688290357589722    loss2: 1.5926066637039185    loss3: 0.1473134309053421\n",
      "On source domain: Epoch 81/161  accuracy: 0.999 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 81/161  accuracy: 0.9001116071428571\n",
      "-------------------------------------------------\n",
      "loss1: 1.4689624309539795    loss2: 1.5879424810409546    loss3: 0.12660929560661316\n",
      "loss1: 1.4611518383026123    loss2: 1.8034355640411377    loss3: 0.11060354113578796\n",
      "On source domain: Epoch 91/161  accuracy: 0.999 \n",
      "loss1: 1.4615380764007568    loss2: 1.3814400434494019    loss3: 0.1866358071565628\n",
      "loss1: 1.4611510038375854    loss2: 1.1612273454666138    loss3: 0.13320407271385193\n",
      "On source domain: Epoch 101/161  accuracy: 1.000 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 101/161  accuracy: 0.9129464285714286\n",
      "-------------------------------------------------\n",
      "loss1: 1.4611507654190063    loss2: 1.6535686254501343    loss3: 0.1311294287443161\n",
      "loss1: 1.461151361465454    loss2: 1.3958582878112793    loss3: 0.12415338307619095\n",
      "On source domain: Epoch 111/161  accuracy: 1.000 \n",
      "loss1: 1.4611531496047974    loss2: 1.6774566173553467    loss3: 0.0872407853603363\n",
      "loss1: 1.4611507654190063    loss2: 1.2746543884277344    loss3: 0.36677661538124084\n",
      "On source domain: Epoch 121/161  accuracy: 1.000 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 121/161  accuracy: 0.9157366071428571\n",
      "-------------------------------------------------\n",
      "loss1: 1.4611544609069824    loss2: 1.2514359951019287    loss3: 0.22747401893138885\n",
      "loss1: 1.461154818534851    loss2: 1.249685287475586    loss3: 0.2552720606327057\n",
      "On source domain: Epoch 131/161  accuracy: 1.000 \n",
      "loss1: 1.4611507654190063    loss2: 1.3341010808944702    loss3: 0.33058130741119385\n",
      "loss1: 1.4611510038375854    loss2: 1.2864140272140503    loss3: 0.0729578360915184\n",
      "On source domain: Epoch 141/161  accuracy: 1.000 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 141/161  accuracy: 0.9291294642857143\n",
      "-------------------------------------------------\n",
      "loss1: 1.4611531496047974    loss2: 1.1606805324554443    loss3: 0.2258576601743698\n",
      "loss1: 1.4611759185791016    loss2: 1.242147445678711    loss3: 0.036586422473192215\n",
      "On source domain: Epoch 151/161  accuracy: 1.000 \n",
      "loss1: 1.4611510038375854    loss2: 1.1782015562057495    loss3: 0.09399006515741348\n",
      "loss1: 1.4611597061157227    loss2: 1.0720632076263428    loss3: 0.14257247745990753\n",
      "On source domain: Epoch 161/161  accuracy: 1.000 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 161/161  accuracy: 0.9229910714285714\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# for epoch in tqdm(range(n_epoch)):\n",
    "for epoch in range(n_epoch):\n",
    "    \n",
    "    for data,labels in train_dataloader:\n",
    "        data=data.to(device)\n",
    "        labels=labels.to(device)\n",
    "        # print(labels)\n",
    "        X_t = X_t.to(device)\n",
    "        Y_t = Y_t.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        map_s = encoder(data)\n",
    "        y_pred=classifier(map_s)\n",
    "        loss1=loss_fn1(y_pred,labels)\n",
    "        map_t = encoder(X_t)\n",
    "        \n",
    "        loss2 = 0\n",
    "        loss3 = 0\n",
    "        means_s = []\n",
    "        # means_t = []\n",
    "        \n",
    "        for num in range(10):\n",
    "#             subset = map_t[Y_t == num]\n",
    "#             means_t.append(torch.mean(subset, dim = 0))\n",
    "            subset = map_s[labels == num]\n",
    "            if len(subset) <= 1: \n",
    "                danger = 1\n",
    "            means_s.append(torch.mean(subset, dim = 0))\n",
    "        for ctr in range(10*n_support):\n",
    "            num = Y_t[ctr]\n",
    "            dd = torch.stack([ map_t[ctr] - means_s[num] ]*10)\n",
    "            Cplane = torch.stack( [(means_s[i-1] - means_s[i]) for i in range(10)] )\n",
    "            loss2 += loss_fn2(-dd, Cplane, torch.FloatTensor([-1]*10).to(device))\n",
    "            loss2 += loss_fn2(dd, Cplane, torch.FloatTensor([-1]*10).to(device))\n",
    "            Cplane = torch.stack( [( map_t[ctr] - means_s[i] ) for i in range(10) if i != num] )\n",
    "            loss3 += loss_fn3(dd[:-1], Cplane, torch.FloatTensor([-1]*9).to(device))\n",
    "        if torch.isnan(loss1) or torch.isnan(loss2) or torch.isnan(loss3):\n",
    "            continue\n",
    "        loss = loss1 + gamma*loss2/(10*n_support) + theta*loss3/(10*n_support)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch%5 == 0: print(\"loss1:\", loss1.item(), \"   loss2:\", loss2.item(), \"   loss3:\", loss3.item())\n",
    "        \n",
    "    if epoch%10 == 0:\n",
    "        acc=0\n",
    "        for data,labels in test_dataloader:\n",
    "            data=data.to(device)\n",
    "            labels=labels.to(device)\n",
    "            y_test_pred=classifier(encoder(data))\n",
    "            acc+=(torch.max(y_test_pred,1)[1]==labels).float().mean().item()\n",
    "        accuracy=round(acc / float(len(test_dataloader)), 3)\n",
    "        print(\"On source domain: Epoch %d/%d  accuracy: %.3f \"%(epoch+1,n_epoch,accuracy))\n",
    "    \n",
    "    if epoch%20 == 0:\n",
    "        mapset_f = []\n",
    "        labelset_f = []\n",
    "        for data, labels in train_dataloader:\n",
    "            data = data.to(device)\n",
    "            labels=labels.to(device)\n",
    "            map_f = encoder(data)\n",
    "            mapset_f.append(map_f)\n",
    "            labelset_f.append(labels)\n",
    "        map_f = torch.cat(mapset_f[:-1])\n",
    "        label_f = torch.cat(labelset_f[:-1])\n",
    "\n",
    "        means_f = []\n",
    "        for num in range(10):\n",
    "            subset = map_f[label_f == num]\n",
    "            means_f.append(torch.mean(subset, dim = 0))\n",
    "#        nume = 0\n",
    "        deno = 0\n",
    "        acc = 0  \n",
    "        for data, labels in real_test_loader:\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)        \n",
    "            map_ff = encoder(data)\n",
    "            distTS = []\n",
    "            for ii in range(10):\n",
    "                distTS.append(torch.norm((map_ff - means_f[ii]), dim=1))\n",
    "            distTS = torch.stack(distTS)\n",
    "            acc+=torch.sum(torch.argmin(distTS, dim=0)==labels)\n",
    "            \n",
    "#             for ctr in range(batch_size_test):\n",
    "#                 others = []\n",
    "#                 for j in range(10):\n",
    "#                     num = j\n",
    "#                     tmp = map_ff[ctr] - means_f[num]\n",
    "#                     dd_f = torch.stack([tmp]*9)\n",
    "#                     tmp = [(means_f[num] - means_f[i]) for i in range(10) if i != num]\n",
    "#                     Cplane_f = torch.stack(tmp)\n",
    "#                     loss_f = loss_fn2(-dd_f, Cplane_f, torch.FloatTensor([-1]*9).to(device)) + loss_fn2(dd_f, Cplane_f, torch.FloatTensor([-1]*9).to(device))\n",
    "#                     others.append(loss_f.item())\n",
    "#                 # print(min(others))\n",
    "#                 if np.argmin(others) == labels[ctr]: \n",
    "#                     nume+=1\n",
    "            deno += len(labels)\n",
    "        print(\"-------------------------------------------------\")\n",
    "#        print(\"On target domain: Epoch %d/%d  accuracy:\"%(epoch+1,n_epoch), nume / deno)\n",
    "        print(\"Another one on TD: Epoch %d/%d  accuracy:\"%(epoch+1,n_epoch), acc.item() / deno)\n",
    "        print(\"-------------------------------------------------\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1: 1.461153268814087    loss2: 0.9428699612617493    loss3: 0.19058966636657715\n",
      "loss1: 1.4611526727676392    loss2: 1.050830602645874    loss3: 0.13861379027366638\n",
      "On source domain: Epoch 11/161  accuracy: 1.000 \n",
      "loss1: 1.461152195930481    loss2: 1.033910870552063    loss3: 0.0010127954883500934\n",
      "loss1: 1.4611756801605225    loss2: 0.8750697374343872    loss3: 0.08127082884311676\n",
      "On source domain: Epoch 21/161  accuracy: 1.000 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 21/161  accuracy: 0.93359375\n",
      "-------------------------------------------------\n",
      "loss1: 1.4611507654190063    loss2: 0.7994027137756348    loss3: 0.010811077430844307\n",
      "loss1: 1.465362548828125    loss2: 1.006070852279663    loss3: 0.013372169807553291\n",
      "On source domain: Epoch 31/161  accuracy: 1.000 \n",
      "loss1: 1.4611573219299316    loss2: 1.1495729684829712    loss3: 0.253337025642395\n",
      "loss1: 1.4612082242965698    loss2: 0.9498468637466431    loss3: 0.0390334390103817\n",
      "On source domain: Epoch 41/161  accuracy: 1.000 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 41/161  accuracy: 0.9380580357142857\n",
      "-------------------------------------------------\n",
      "loss1: 1.4611507654190063    loss2: 0.8849682807922363    loss3: 0.028814733028411865\n",
      "loss1: 1.4611507654190063    loss2: 0.955994188785553    loss3: 0.15319931507110596\n",
      "On source domain: Epoch 51/161  accuracy: 1.000 \n",
      "loss1: 1.461150884628296    loss2: 0.979511559009552    loss3: 0.02699683979153633\n",
      "loss1: 1.4611507654190063    loss2: 0.9031352996826172    loss3: 0.14931923151016235\n",
      "On source domain: Epoch 61/161  accuracy: 1.000 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 61/161  accuracy: 0.9358258928571429\n",
      "-------------------------------------------------\n",
      "loss1: 1.4611579179763794    loss2: 0.8331054449081421    loss3: 0.05307163670659065\n",
      "loss1: 1.4611831903457642    loss2: 0.9454361796379089    loss3: 0.008610003627836704\n",
      "On source domain: Epoch 71/161  accuracy: 1.000 \n",
      "loss1: 1.4612302780151367    loss2: 0.861905038356781    loss3: 0.004016717430204153\n",
      "loss1: 1.4611507654190063    loss2: 0.893081784248352    loss3: 0.10141494125127792\n",
      "On source domain: Epoch 81/161  accuracy: 1.000 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 81/161  accuracy: 0.9352678571428571\n",
      "-------------------------------------------------\n",
      "loss1: 1.4611834287643433    loss2: 0.7712298631668091    loss3: 0.05097084492444992\n",
      "loss1: 1.4612034559249878    loss2: 0.7494073510169983    loss3: 0.04370269924402237\n",
      "On source domain: Epoch 91/161  accuracy: 1.000 \n",
      "loss1: 1.4611552953720093    loss2: 0.9072171449661255    loss3: 0.12037115544080734\n",
      "loss1: 1.4611507654190063    loss2: 0.8857889771461487    loss3: 0.03267744556069374\n",
      "On source domain: Epoch 101/161  accuracy: 1.000 \n",
      "-------------------------------------------------\n",
      "Another one on TD: Epoch 101/161  accuracy: 0.9391741071428571\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# for epoch in tqdm(range(n_epoch)):\n",
    "for epoch in range(101):\n",
    "    \n",
    "    for data,labels in train_dataloader:\n",
    "        data=data.to(device)\n",
    "        labels=labels.to(device)\n",
    "        # print(labels)\n",
    "        X_t = X_t.to(device)\n",
    "        Y_t = Y_t.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        map_s = encoder(data)\n",
    "        y_pred=classifier(map_s)\n",
    "        loss1=loss_fn1(y_pred,labels)\n",
    "        map_t = encoder(X_t)\n",
    "        \n",
    "        loss2 = 0\n",
    "        loss3 = 0\n",
    "        means_s = []\n",
    "        # means_t = []\n",
    "        \n",
    "        for num in range(10):\n",
    "#             subset = map_t[Y_t == num]\n",
    "#             means_t.append(torch.mean(subset, dim = 0))\n",
    "            subset = map_s[labels == num]\n",
    "            if len(subset) <= 1: \n",
    "                danger = 1\n",
    "            means_s.append(torch.mean(subset, dim = 0))\n",
    "        for ctr in range(10*n_support):\n",
    "            num = Y_t[ctr]\n",
    "            dd = torch.stack([ map_t[ctr] - means_s[num] ]*10)\n",
    "            Cplane = torch.stack( [(means_s[i-1] - means_s[i]) for i in range(10)] )\n",
    "            loss2 += loss_fn2(-dd, Cplane, torch.FloatTensor([-1]*10).to(device))\n",
    "            loss2 += loss_fn2(dd, Cplane, torch.FloatTensor([-1]*10).to(device))\n",
    "            Cplane = torch.stack( [( map_t[ctr] - means_s[i] ) for i in range(10) if i != num] )\n",
    "            loss3 += loss_fn3(dd[:-1], Cplane, torch.FloatTensor([-1]*9).to(device))\n",
    "        if torch.isnan(loss1) or torch.isnan(loss2) or torch.isnan(loss3):\n",
    "            continue\n",
    "        loss = loss1 + gamma*loss2/(10*n_support) + theta*loss3/(10*n_support)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch%5 == 0: print(\"loss1:\", loss1.item(), \"   loss2:\", loss2.item(), \"   loss3:\", loss3.item())\n",
    "        \n",
    "    if epoch%10 == 0:\n",
    "        acc=0\n",
    "        for data,labels in test_dataloader:\n",
    "            data=data.to(device)\n",
    "            labels=labels.to(device)\n",
    "            y_test_pred=classifier(encoder(data))\n",
    "            acc+=(torch.max(y_test_pred,1)[1]==labels).float().mean().item()\n",
    "        accuracy=round(acc / float(len(test_dataloader)), 3)\n",
    "        print(\"On source domain: Epoch %d/%d  accuracy: %.3f \"%(epoch+1,n_epoch,accuracy))\n",
    "    \n",
    "    if epoch%20 == 0:\n",
    "        mapset_f = []\n",
    "        labelset_f = []\n",
    "        for data, labels in train_dataloader:\n",
    "            data = data.to(device)\n",
    "            labels=labels.to(device)\n",
    "            map_f = encoder(data)\n",
    "            mapset_f.append(map_f)\n",
    "            labelset_f.append(labels)\n",
    "        map_f = torch.cat(mapset_f[:-1])\n",
    "        label_f = torch.cat(labelset_f[:-1])\n",
    "\n",
    "        means_f = []\n",
    "        for num in range(10):\n",
    "            subset = map_f[label_f == num]\n",
    "            means_f.append(torch.mean(subset, dim = 0))\n",
    "#        nume = 0\n",
    "        deno = 0\n",
    "        acc = 0  \n",
    "        for data, labels in real_test_loader:\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)        \n",
    "            map_ff = encoder(data)\n",
    "            distTS = []\n",
    "            for ii in range(10):\n",
    "                distTS.append(torch.norm((map_ff - means_f[ii]), dim=1))\n",
    "            distTS = torch.stack(distTS)\n",
    "            acc+=torch.sum(torch.argmin(distTS, dim=0)==labels)\n",
    "            \n",
    "#             for ctr in range(batch_size_test):\n",
    "#                 others = []\n",
    "#                 for j in range(10):\n",
    "#                     num = j\n",
    "#                     tmp = map_ff[ctr] - means_f[num]\n",
    "#                     dd_f = torch.stack([tmp]*9)\n",
    "#                     tmp = [(means_f[num] - means_f[i]) for i in range(10) if i != num]\n",
    "#                     Cplane_f = torch.stack(tmp)\n",
    "#                     loss_f = loss_fn2(-dd_f, Cplane_f, torch.FloatTensor([-1]*9).to(device)) + loss_fn2(dd_f, Cplane_f, torch.FloatTensor([-1]*9).to(device))\n",
    "#                     others.append(loss_f.item())\n",
    "#                 # print(min(others))\n",
    "#                 if np.argmin(others) == labels[ctr]: \n",
    "#                     nume+=1\n",
    "            deno += len(labels)\n",
    "        print(\"-------------------------------------------------\")\n",
    "#        print(\"On target domain: Epoch %d/%d  accuracy:\"%(epoch+1,n_epoch), nume / deno)\n",
    "        print(\"Another one on TD: Epoch %d/%d  accuracy:\"%(epoch+1,n_epoch), acc.item() / deno)\n",
    "        print(\"-------------------------------------------------\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.23879255e-03 5.22287451e-02 9.59805213e-03 3.86472861e-03\n",
      " 3.61749339e+00 1.08772459e+01 1.68376043e-02 3.06823780e-03\n",
      " 7.96256065e-02 5.04932739e-03 5.27169928e-03 1.30827236e+01\n",
      " 2.79178098e-03 1.87596083e-01 2.74690171e-03 8.21155262e+00\n",
      " 6.30398607e+00 9.01248958e-03 1.49523973e+01 5.73888254e+00\n",
      " 7.42509794e+00 7.69134844e-04 3.46922083e-03 1.02345459e-02\n",
      " 8.17765713e+00 5.01655787e-03 6.57811761e-02 7.07086001e-04\n",
      " 1.82856526e-02 9.46878397e-04 1.05989046e+01 1.10120687e+01\n",
      " 4.83924113e-02 9.65701199e+00 8.65636952e-03 7.36839008e+00\n",
      " 3.64728928e+00 1.82187872e-03 9.03719556e-05 7.35651629e-05\n",
      " 4.03609499e-03 3.06630530e-03 9.86695576e+00 2.26608276e+00\n",
      " 2.35741446e-03 2.42280774e-03 4.69894335e-02 3.59939504e-03\n",
      " 1.16868806e-03 3.41249455e-04 1.18673115e+01 1.00807764e-03\n",
      " 3.05456114e+00 8.20095253e+00 6.77808762e+00 1.06108002e-02\n",
      " 7.24169798e-03 3.85132022e-02 4.85484409e+00 2.05113064e-03\n",
      " 2.43703742e-02 2.06792774e-03 3.07185412e-03 0.00000000e+00\n",
      " 4.32378333e-03 1.49357710e+01 2.61351466e-03 3.20753269e-03\n",
      " 3.09946621e-03 3.12308810e-04 4.27587223e+00 3.30227522e-05\n",
      " 1.18284142e-02 3.34488833e-03 1.05761562e-03 4.44738102e+00\n",
      " 6.28662412e-04 5.96168824e-03 4.15735273e-03 4.56796587e-02\n",
      " 3.37444592e+00 2.99481638e-02 4.44471557e-03 4.28944778e+00]\n",
      "0.0,\n",
      "49.89803,0.0,\n",
      "49.83314,40.686764,0.0,\n",
      "51.26317,43.45977,41.498558,0.0,\n",
      "46.94788,39.32698,39.299873,41.571053,0.0,\n",
      "51.16788,45.102287,45.894283,45.549637,40.300766,0.0,\n",
      "50.06555,43.550903,44.028263,46.40181,38.892544,43.97023,0.0,\n",
      "49.46111,42.305367,41.863255,44.24626,38.612965,44.808407,43.651855,0.0,\n",
      "49.167046,41.53952,41.90925,43.962193,38.678394,44.73572,43.460438,41.566902,0.0,\n",
      "43.907085,38.753597,39.93501,40.547543,32.10385,40.688026,41.030228,37.206524,36.742336,0.0,\n"
     ]
    }
   ],
   "source": [
    "# result check\n",
    "mapset = []\n",
    "labelset = []\n",
    "for data, labels in train_dataloader:\n",
    "    data=data.to(device)\n",
    "    fmap = encoder(data).cpu().detach().numpy()\n",
    "    labels=labels.to(device).cpu().detach().numpy()\n",
    "    mapset.append(fmap)\n",
    "    labelset.append(labels)\n",
    "\n",
    "smap = np.vstack(mapset[:-1])\n",
    "slabel = np.hstack(labelset[:-1])\n",
    "\n",
    "means = []\n",
    "dmeans = []\n",
    "\n",
    "for num in range(10):\n",
    "    subset1 = smap[slabel == num]\n",
    "    means1 = np.mean(subset1, axis=0)\n",
    "    tmp = subset1 - means1\n",
    "    dists1 = np.linalg.norm(tmp, axis=1)\n",
    "    means.append(means1)\n",
    "    dmeans.append(np.mean(dists1))\n",
    "# print(means[0])\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(i+1):\n",
    "        print(np.linalg.norm(means[i] - means[j]), end=',')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.239137, 10.070674, 14.430747, 12.062499, 9.656775, 12.180727, 9.247488, 11.649317, 10.3363905, 8.067777]\n"
     ]
    }
   ],
   "source": [
    "tmapset = []\n",
    "tlabelset = []\n",
    "for data, labels in real_test_loader:\n",
    "    data=data.to(device)\n",
    "    fmap = encoder(data).cpu().detach().numpy()\n",
    "    labels=labels.to(device).cpu().detach().numpy()\n",
    "    tmapset.append(fmap)\n",
    "    tlabelset.append(labels)\n",
    "\n",
    "tmap = np.vstack(tmapset[:-1])\n",
    "tlabel = np.hstack(tlabelset[:-1])\n",
    "\n",
    "tmeans = []\n",
    "tdmeans = []\n",
    "for num in range(10):\n",
    "    subset1 = tmap[tlabel == num]\n",
    "    means1 = np.mean(subset1, axis=0)\n",
    "    tmp = subset1 - means1\n",
    "    dists1 = np.linalg.norm(tmp, axis=1)\n",
    "    tmeans.append(means1)\n",
    "    tdmeans.append(np.mean(dists1))\n",
    "print(dmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39.3769, 59.36251, 59.26669, 60.47117, 56.73069, 60.30749, 59.089848, 59.025127, 58.65511, 54.43972]\n",
      "[57.060223, 30.760845, 49.41133, 51.64115, 48.102978, 52.98966, 51.64625, 50.552414, 49.859398, 47.463226]\n",
      "[52.957363, 44.977097, 28.1175, 46.078617, 43.06595, 49.11844, 47.42685, 45.62019, 45.558594, 43.595142]\n",
      "[57.84237, 51.6468, 49.69099, 36.426094, 49.460583, 52.86915, 53.604626, 51.795273, 51.22441, 48.475395]\n",
      "[52.19363, 45.416985, 45.487587, 47.445435, 27.51278, 46.69033, 45.324505, 44.57756, 43.937416, 38.480984]\n",
      "[54.28899, 48.70962, 49.12572, 48.290226, 44.866276, 34.65721, 47.97307, 48.686527, 48.04868, 44.503345]\n",
      "[57.05126, 50.918297, 51.772343, 53.702095, 47.700718, 51.711727, 33.251354, 51.614666, 50.984833, 49.11289]\n",
      "[55.980247, 49.579895, 49.270756, 51.59235, 46.580143, 52.01401, 51.11465, 32.4685, 49.11644, 45.28119]\n",
      "[55.622772, 48.26707, 49.037125, 50.8245, 46.03742, 51.419674, 50.37614, 48.91806, 33.908077, 44.40021]\n",
      "[51.362907, 46.026337, 46.77273, 47.708145, 39.616318, 47.66194, 47.578457, 43.564087, 43.738506, 31.136797]\n"
     ]
    }
   ],
   "source": [
    "for num in range(10):\n",
    "    subset1 = tmap[tlabel == num]\n",
    "    tsd = []\n",
    "    for i in range(10):\n",
    "        tmp = np.linalg.norm((subset1 - means[i]), axis=1)\n",
    "        tsd.append(np.mean(tmp))\n",
    "    print(tsd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
